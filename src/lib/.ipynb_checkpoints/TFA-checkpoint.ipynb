{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "module that implement Topographic Factor Analysis (TFA) model described in \n",
    "Manning, Jeremy R., et al. \"Topographic factor analysis: a Bayesian model for inferring brain networks from neural data.\" \n",
    "PloS one 9.5 (2014).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division \n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from copy import deepcopy\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperparameter(location_arr):\n",
    "    '''\n",
    "    set the fixed hyperparamters \\pi\n",
    "    based on Table 2 in the original paper\n",
    "    location_arr: numpy.nd array, size no_voxels X dim\n",
    "    ---------------------\n",
    "    Return:\n",
    "    a dict, keys are names of the hyperparameters\n",
    "    values are cossponding fixed value\n",
    "    '''\n",
    "    pi_keys = ['sigma_y', 'mu_w', 'k_w', 'c', 'k_mu', 'mu_lambda', 'k_lambda']\n",
    "    pi = {key: None for key in pi_keys}\n",
    "    pi['sigma_y'] = 0.1\n",
    "    pi['mu_w'] = 0\n",
    "    pi['k_w'] = np.log(0.5)\n",
    "    pi['c'] = np.mean(location_arr, axis=0)\n",
    "    sigma_mu = np.var(location_arr, axis=0)\n",
    "    pi['k_mu'] = np.log(1/(10*sigma_mu))\n",
    "    pi['mu_lambda'] = 1\n",
    "    pi['k_lambda'] = np.log(1/3)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variationalparameter():\n",
    "    '''\n",
    "    a dict representing variational parameter alpha\n",
    "    mu_w_n_k, k_w_n_k: size N X K\n",
    "    k_mu_k, mu_mu_k: K X D\n",
    "    rest: K\n",
    "    ----------------------\n",
    "    Return:\n",
    "    a dict, keys are names of the variational parameters\n",
    "    values are none\n",
    "    '''\n",
    "    alpha_keys = ['mu_w_n_k', 'k_w_n_k', 'mu_mu_k', 'k_mu_k', 'mu_lambda_k', 'k_lambda_k']\n",
    "    alpha = {key: None for key in alpha_keys}\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_from_normal(mu, k):\n",
    "    '''\n",
    "    random sample a point in one dimensional normal distribution \n",
    "    with mean=mu, var=exp(-k)\n",
    "    '''\n",
    "    return np.random.normal(mu, np.sqrt(np.exp(-k)),1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initial_alpha(pi, N, K):\n",
    "    '''\n",
    "    initialize variational parameters alpha based on section 1.4\n",
    "    note, this is NOT a implementation of hotspot initialization on section 1.4.1\n",
    "    pi: dict, hyperparameter\n",
    "    N: number of images\n",
    "    K: number of source\n",
    "    --------------\n",
    "    Return:\n",
    "    dict, with initialized alpha values in dict\n",
    "    '''\n",
    "    alpha = variationalparameter()\n",
    "    alpha['k_w_n_k'] = np.ones((N,K))*np.log(10)\n",
    "    alpha['k_lambda_k'] = np.ones(K)*1\n",
    "    D = len(pi['k_mu'])\n",
    "    alpha['k_mu_k'] = np.ones((K, D))* (pi['k_mu']+3*np.log(10))\n",
    "    value_mu_mu_k = np.empty((K, D))\n",
    "    for k in np.arange(K):\n",
    "        value_mu_mu_k[k,:] = [sample_from_normal(c_i, k_i) for (c_i, k_i) in zip(pi['c'], pi['k_mu'])]\n",
    "    alpha['mu_mu_k'] = value_mu_mu_k\n",
    "    alpha['mu_lambda_k'] = np.array([sample_from_normal(pi['mu_lambda'], pi['k_lambda']) for _ in np.arange(K)])\n",
    "    alpha['mu_w_n_k'] = np.array([sample_from_normal(pi['mu_w'], pi['k_w']) for _ in np.arange(N*K)]).reshape(N,K)\n",
    "    return alpha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initial_eta(alpha):\n",
    "    '''\n",
    "    init eta dict, this is similar as alpha\n",
    "    '''\n",
    "    eta_keys = alpha.keys()\n",
    "    eta_values_shape = iter([v.shape for v in alpha.values()])\n",
    "    eta = {}\n",
    "    for key in eta_keys:\n",
    "        eta[key] = np.ones(eta_values_shape.next())\n",
    "    return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sub_sampling_mask(N, V, N_sub, V_sub):\n",
    "    '''\n",
    "    sub sample part of the image to compute posterior\n",
    "    this is described in section 1.6\n",
    "    ------------------\n",
    "    Return:\n",
    "    binary mask array with size (N, V)\n",
    "    '''\n",
    "    sub_mask = np.zeros((N, V))\n",
    "    assert N_sub <= N\n",
    "    assert V_sub <= V \n",
    "    sub_sample_image_index = np.random.choice(N, N_sub, replace=False)\n",
    "    start_point = np.random.randint(V-V_sub+1)\n",
    "    for n in sub_sample_image_index:\n",
    "        sub_mask[n,start_point:(start_point+V_sub)] = 1\n",
    "    return sub_mask, sub_sample_image_index, start_point\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_sample_from_Q():\n",
    "    '''\n",
    "    a dict represents samples from q, it will include 3 keys\n",
    "    mu_k_d: K X D\n",
    "    k_lambda: K\n",
    "    w_n_sub_k: Nsub X K\n",
    "    '''\n",
    "    sample_keys = ['mu_k_d', 'k_lambda', 'w_n_sub_k']\n",
    "    sample = {key: None for key in sample_keys}\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_from_Q(alpha_0, sub_sample_image_index):\n",
    "    '''\n",
    "    Sample M samples from variantional distribution \n",
    "    this is described in S1, 1.7\n",
    "    -------------------\n",
    "    Return:\n",
    "    a simple dict with three keys\n",
    "    '''\n",
    "    K = alpha_0['k_mu_k'].shape[0]\n",
    "    D = alpha_0['k_mu_k'].shape[1]\n",
    "    sample = one_sample_from_Q()\n",
    "    sample['mu_k_d'] = np.array([sample_from_normal(mu_k_d, k_mu_k_d) for (mu_k_d, k_mu_k_d) in zip(alpha_0['mu_mu_k'].ravel(), alpha_0['k_mu_k'].ravel())]).reshape((K,D))\n",
    "    sample['k_lambda'] = np.array([sample_from_normal(mu_lambda_k, k_lambda_k) for (mu_lambda_k, k_lambda_k) in zip(alpha_0['mu_lambda_k'], alpha_0['k_lambda_k'])])\n",
    "    value_w_n_sub_k = np.zeros((len(sub_sample_image_index), K))\n",
    "    for sub_index, n in enumerate(sub_sample_image_index):\n",
    "        mu = alpha_0['mu_w_n_k'][n,:]\n",
    "        k = alpha_0['k_w_n_k'][n,:]\n",
    "        value_w_n_sub_k[sub_index,:] = [sample_from_normal(mu_i,k_i) for (mu_i, k_i) in zip(mu,k)]\n",
    "    sample['w_n_sub_k'] = value_w_n_sub_k\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RBF(r, mu, Lambda):\n",
    "    '''\n",
    "    RBF kernel function described in equation (1)\n",
    "    and used in equation (3)\n",
    "    ----------------------\n",
    "    Return:\n",
    "    scalar \n",
    "    '''\n",
    "    assert len(r)==len(mu)==3\n",
    "    return np.exp((-(np.dot(np.transpose(r-mu), r-mu))/(np.exp(Lambda))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def F_posterior(sample, location_arr, start_point, V_sub):\n",
    "    '''\n",
    "    F_posterior matrix \n",
    "    --alpha: dictionary, format can be referred from TFA\n",
    "    --voxel: 2d array, V_sub*D, represent all voxels' location\n",
    "    --return: 2d array, K*V_sub\n",
    "    '''\n",
    "    mu_mat = sample['mu_k_d']\n",
    "    lambda_array = sample['k_lambda']\n",
    "    K = mu_mat.shape[0]\n",
    "    voxel = location_arr[start_point:(start_point+V_sub),:]\n",
    "    F_mat = np.empty((K, V_sub))\n",
    "    for i in np.arange(K):\n",
    "        for j in np.arange(V_sub):\n",
    "            F_mat[i,j] = RBF(voxel[j],mu_mat[i],lambda_array[i])\n",
    "    return F_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logP(sample,Y,location_arr,sub_sample_image_index,start_point,V_sub, pi):\n",
    "    '''\n",
    "    log of the joint probability of the data and hidden variables\n",
    "    --sample: dictionary\n",
    "    --Y: observed data\n",
    "    '''\n",
    "    #log p(image)\n",
    "    F_mat = F_posterior(sample,location_arr, start_point, V_sub)\n",
    "    w_mat = sample['w_n_sub_k']\n",
    "    mu_n_sub_v_sub_mat = w_mat.dot(F_mat)\n",
    "    p1 = 0.0\n",
    "    sigma_y = pi['sigma_y'] \n",
    "    for i in np.arange(sub_sample_image_index.shape[0]):\n",
    "        for j in np.arange(V_sub):\n",
    "            p1+= normal_pdf_sigma(Y[sub_sample_image_index[i],(start_point+j)],mu_n_sub_v_sub_mat[i,j],sigma_y)\n",
    "    p1*=location_arr.shape[0]/V_sub\n",
    "    \n",
    "    #log p(weight)\n",
    "    mu_w = pi['mu_w']\n",
    "    k_mu = pi['k_w'] \n",
    "    p2 = np.sum([normal_pdf(w,mu_w, k_mu) for w in w_mat.ravel()])\n",
    "    \n",
    "    # log p(centers)\n",
    "    c = pi['c'] \n",
    "    k_mu = pi['k_mu'] \n",
    "    mu_k = sample['mu_k_d']\n",
    "    p3 = 0.0\n",
    "    for i in np.arange(mu_k.shape[0]):\n",
    "        p3 += np.sum([normal_pdf(x, mu, k) for (x, mu, k) in zip(mu_k[i,:], c, k_mu)])\n",
    "    \n",
    "    #log p(widths)\n",
    "    mu_lambda = pi['mu_lambda']\n",
    "    k_lambda  = pi['k_lambda'] \n",
    "    lambda_k = sample['k_lambda']\n",
    "    p4 = np.sum([normal_pdf(l, mu_lambda, k_lambda) for l in lambda_k ])\n",
    "    \n",
    "    return (p1+p2+p3+p4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logQ(sample, alpha_0, sub_sample_image_index):\n",
    "    '''\n",
    "    compute log Q as described in S1 section 1.2\n",
    "    '''\n",
    "    q1 = 0.0\n",
    "    for sub_index, n in enumerate(sub_sample_image_index):\n",
    "        w = sample['w_n_sub_k'][sub_index, :]\n",
    "        mu = alpha_0['mu_w_n_k'][n,:]\n",
    "        k = alpha_0['k_w_n_k'][n,:]\n",
    "        q1 += np.sum([normal_pdf(w_i, mu_i,k_i) for (w_i, mu_i, k_i) in zip(w, mu,k)])\n",
    "\n",
    "    q2 = np.sum([normal_pdf(mu_k_d_i, mu_mu_k_i, k_mu_k_i) for (mu_k_d_i, mu_mu_k_i, k_mu_k_i) in zip(sample['mu_k_d'].ravel(), alpha_0['mu_mu_k'].ravel(), alpha_0['k_mu_k'].ravel())])\n",
    "    \n",
    "    q3 = np.sum([normal_pdf(k_lambda_i, mu_lambda_k_i, k_lambda_k_i) for (k_lambda_i, mu_lambda_k_i, k_lambda_k_i) in zip(sample['k_lambda'], alpha_0['mu_lambda_k'], alpha_0['k_lambda_k'])])\n",
    "    \n",
    "    return q1+q2+q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_pdf(x, mu, k):\n",
    "    '''\n",
    "    one dimensional normal pdf  \n",
    "    with mean=mu, var=exp(-k)\n",
    "    '''\n",
    "    return np.log(norm.pdf(x, loc=mu, scale=np.sqrt(np.exp(-k))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_pdf_sigma(x, mu, sigma2):\n",
    "    '''\n",
    "    write me!\n",
    "    '''\n",
    "    return np.log(norm.pdf(x,loc=mu, scale=np.sqrt(sigma2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def z1(s,mu,k):\n",
    "    return (s-mu)*np.exp(k)\n",
    "\n",
    "def z2(s,mu,k):\n",
    "    return 0.5 - 0.5*(s-mu)**2*np.exp(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_log_q_w_1(sample, alpha_0, sub_sample_image_index):\n",
    "    '''\n",
    "    to write!\n",
    "    '''\n",
    "    w_n_k = sample['w_n_sub_k']\n",
    "    mu_n_k = alpha_0['mu_w_n_k']\n",
    "    k_w_n_k = alpha_0['k_w_n_k']\n",
    "    N_sub = len(sub_sample_image_index)\n",
    "    K = w_n_k.shape[1]\n",
    "    z1_array = np.zeros((N_sub, K))\n",
    "    for sub_index, n in enumerate(sub_sample_image_index):\n",
    "        w = w_n_k[sub_index,:]\n",
    "        mu = mu_n_k[n,:]\n",
    "        k = k_w_n_k[n,:]\n",
    "        z1_array[sub_index,:] = [z1(w_i, mu_i, k_i) for (w_i, mu_i, k_i) in zip(w, mu, k)]\n",
    "    return z1_array\n",
    "\n",
    "def gradient_log_q_w_2(sample, alpha_0, sub_sample_image_index):\n",
    "    '''\n",
    "    to write!\n",
    "    '''\n",
    "    w_n_k = sample['w_n_sub_k']\n",
    "    mu_n_k = alpha_0['mu_w_n_k']\n",
    "    k_w_n_k = alpha_0['k_w_n_k']\n",
    "    N_sub = len(sub_sample_image_index)\n",
    "    K = w_n_k.shape[1]\n",
    "    z2_array = np.zeros((N_sub, K))\n",
    "    for sub_index, n in enumerate(sub_sample_image_index):\n",
    "        w = w_n_k[sub_index,:]\n",
    "        mu = mu_n_k[n,:]\n",
    "        k = k_w_n_k[n,:]\n",
    "        z2_array[sub_index,:] = [z2(w_i, mu_i, k_i) for (w_i, mu_i, k_i) in zip(w, mu, k)]\n",
    "    return z2_array\n",
    "\n",
    "def gradient_log_q_mu_1(sample, alpha_0):\n",
    "    '''\n",
    "    to write!\n",
    "    '''\n",
    "    mu_k_d = sample['mu_k_d']\n",
    "    K = mu_k_d.shape[0]\n",
    "    D = mu_k_d.shape[1]\n",
    "    mu_mu_k_d = alpha_0['mu_mu_k'] \n",
    "    k_mu_k_d = alpha_0['k_mu_k']\n",
    "    z1_array = np.array([z1(mu_k_d_i, mu_mu_k_d_i, k_mu_k_d_i) for(mu_k_d_i, mu_mu_k_d_i, k_mu_k_d_i) in \n",
    "                         zip(mu_k_d.ravel(), k_mu_k_d.ravel(), k_mu_k_d.ravel())]).reshape(K,D)\n",
    "    \n",
    "    return z1_array\n",
    "\n",
    "def gradient_log_q_mu_2(sample, alpha_0):\n",
    "    '''\n",
    "    to write!\n",
    "    '''\n",
    "    mu_k_d = sample['mu_k_d']\n",
    "    K = mu_k_d.shape[0]\n",
    "    D = mu_k_d.shape[1]\n",
    "    mu_mu_k_d = alpha_0['mu_mu_k'] \n",
    "    k_mu_k_d = alpha_0['k_mu_k']\n",
    "    z2_array = np.array([z2(mu_k_d_i, mu_mu_k_d_i, k_mu_k_d_i) for(mu_k_d_i, mu_mu_k_d_i, k_mu_k_d_i) in \n",
    "                         zip(mu_k_d.ravel(), k_mu_k_d.ravel(), k_mu_k_d.ravel())]).reshape(K,D)\n",
    "    \n",
    "    return z2_array\n",
    "\n",
    "def gradient_log_q_lambda_1(sample, alpha_0):\n",
    "    '''\n",
    "    to write!\n",
    "    '''\n",
    "    lambda_k = sample['k_lambda']\n",
    "    mu_lambda_k = alpha_0['mu_lambda_k']\n",
    "    k_lambda_k = alpha_0['k_lambda_k']\n",
    "    z1_array = np.array([z1(l,mu,k) for (l,mu,k) in zip(lambda_k, mu_lambda_k, k_lambda_k)])\n",
    "    return z1_array\n",
    "\n",
    "def gradient_log_q_lambda_2(sample, alpha_0):\n",
    "    '''\n",
    "    to write!\n",
    "    '''\n",
    "    lambda_k = sample['k_lambda']\n",
    "    mu_lambda_k = alpha_0['mu_lambda_k']\n",
    "    k_lambda_k = alpha_0['k_lambda_k']\n",
    "    z2_array = np.array([z2(l,mu,k) for (l,mu,k) in zip(lambda_k, mu_lambda_k, k_lambda_k)])\n",
    "    return z2_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logPQ(Samples,Y,location_arr,sub_sample_image_index,start_point,V_sub, pi,alpha_0):\n",
    "    logPQ_dic = {}\n",
    "    for index in np.arange(len(Samples)):\n",
    "        log_p = logP(Samples[index], Y,location_arr,sub_sample_image_index,start_point,V_sub, pi)\n",
    "        log_q = logQ(Samples[index], alpha_0, sub_sample_image_index)\n",
    "        logPQ_dic.update({index: [log_p, log_q]})\n",
    "    return logPQ_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ELBO(logPQ_dic):\n",
    "    ELBO = 0\n",
    "    M = len(logPQ_dic)\n",
    "    for index in np.arange(M):\n",
    "        ELBO += (logPQ_dic[index][0] - logPQ_dic[index][1])\n",
    "    ELBO/=M\n",
    "    return ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name_fun_mapping(key):\n",
    "    '''\n",
    "    map string variable name (i.e. keys in alpha_0) to specific function to compute gradient\n",
    "    --------------\n",
    "    Return: fun\n",
    "    '''\n",
    "    mapping = {'mu_w_n_k': gradient_log_q_w_1, 'k_w_n_k': gradient_log_q_w_2,\n",
    "           'mu_mu_k': gradient_log_q_mu_1, 'k_mu_k': gradient_log_q_mu_2,\n",
    "           'mu_lambda_k': gradient_log_q_lambda_1, 'k_lambda_k': gradient_log_q_lambda_2}\n",
    "    return mapping[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def g_h(key, sample, alpha_0, Y,location_arr,sub_sample_image_index,start_point,V_sub, pi):\n",
    "    '''\n",
    "    compute g term for each random sample, this is described in table 5\n",
    "    '''\n",
    "    h_m_fun = name_fun_mapping(key)\n",
    "    fun_args = (sample, alpha_0, sub_sample_image_index) if key in ['mu_w_n_k', 'k_w_n_k'] else (sample, alpha_0)\n",
    "    h_m = h_m_fun(*fun_args)\n",
    "    log_p = logP(sample, Y,location_arr,sub_sample_image_index,start_point,V_sub, pi)\n",
    "    log_q = logQ(sample, alpha_0, sub_sample_image_index)\n",
    "    return (h_m * (log_p - log_q), h_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def g_h_alter(key, sample, t, logPQ_dic, alpha_0, Y,location_arr,sub_sample_image_index,start_point,V_sub, pi):\n",
    "    '''\n",
    "    compute g term for each random sample, this is described in table 5\n",
    "    '''\n",
    "    h_m_fun = name_fun_mapping(key)\n",
    "    fun_args = (sample, alpha_0, sub_sample_image_index) if key in ['mu_w_n_k', 'k_w_n_k'] else (sample, alpha_0)\n",
    "    h_m = h_m_fun(*fun_args)\n",
    "    log_p = logPQ_dic[t][0]\n",
    "    log_q = logPQ_dic[t][1]\n",
    "    return (h_m * (log_p - log_q), h_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def G_H(key, M_samples, logPQ_dic, alpha_0, Y,location_arr,sub_sample_image_index,start_point,V_sub, pi):\n",
    "    '''\n",
    "    return G, H list for all M samples\n",
    "    '''\n",
    "    G_H_list = []\n",
    "    #for check\n",
    "    t = 0\n",
    "    for sample in M_samples:\n",
    "        #print 'Sample {}...'.format(t)\n",
    "        #G_H_list.append(g_h(key, sample, alpha_0, Y,location_arr,sub_sample_image_index,start_point,V_sub, pi))\n",
    "        G_H_list.append(g_h_alter(key, sample, t, logPQ_dic, alpha_0, Y,location_arr,sub_sample_image_index,start_point,V_sub, pi))\n",
    "        t= t+1\n",
    "        \n",
    "    G_list = list(zip(*G_H_list)[0])\n",
    "    H_list = list(zip(*G_H_list)[1])\n",
    "    return (G_list, H_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Beta(G_list,H_list):\n",
    "    '''\n",
    "    Beta is control variate:\n",
    "    '''\n",
    "    raveled_G = np.array([g.ravel() for g in G_list])\n",
    "    raveled_H = np.array([h.ravel() for h in H_list])\n",
    "    assert raveled_G.shape == raveled_H.shape\n",
    "    U = raveled_G.shape[1]\n",
    "    up = 0.0\n",
    "    down = 0.0\n",
    "    for i in np.arange(U):\n",
    "        up += np.cov(raveled_G[:,i], raveled_H[:,i])[0,0]\n",
    "        down += np.cov(raveled_G[:,i]).item()\n",
    "    return up/down\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_estimate(key,M_samples,logPQ_dic,alpha_0,Y,location_arr,sub_sample_image_index,start_point,V_sub, pi):\n",
    "    '''\n",
    "    gradient estimate after collrolling variate\n",
    "    '''\n",
    "    G_list, H_list = G_H(key, M_samples, logPQ_dic, alpha_0, Y,location_arr,sub_sample_image_index,start_point,V_sub, pi)\n",
    "    beta = Beta(G_list, H_list)\n",
    "    #print beta\n",
    "    assert G_list[0].shape == H_list[0].shape\n",
    "    assert len(G_list) == len(H_list)\n",
    "    grad_estimate = np.zeros(G_list[0].shape)\n",
    "    for m in np.arange(len(M_samples)):\n",
    "        grad_estimate += G_list[m]-beta*H_list[m]\n",
    "    return grad_estimate/len(M_samples)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delta_scalar(x,maxStepSize):\n",
    "    '''\n",
    "    compute the final delta to gradient, as described in Table 3\n",
    "    '''\n",
    "    return max(min(x, maxStepSize), -maxStepSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delta_on_grad_estimate(grad_estimate, rho, maxStepSize, sub_sample_image_index):\n",
    "    '''\n",
    "    compute delta update for any gradient estimate\n",
    "    '''\n",
    "    delta = np.vectorize(delta_scalar)\n",
    "    try:\n",
    "        return delta(rho*grad_estimate, maxStepSize)\n",
    "    except ValueError:\n",
    "        return delta(rho[sub_sample_image_index,:]*grad_estimate, maxStepSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_alpha(key, alpha, alpha_0, delta, sub_sample_image_index):\n",
    "    '''\n",
    "    update alpha after looping through one key in alpha_0\n",
    "    '''\n",
    "    try:\n",
    "        alpha[key] = alpha_0[key] + delta\n",
    "    except ValueError:\n",
    "        alpha[key][sub_sample_image_index,:] = alpha_0[key][sub_sample_image_index,:] + delta\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_eta(key, eta, grad_estimate, sub_sample_image_index):\n",
    "    '''\n",
    "    update eta after looping through one key in alpha_0\n",
    "    '''\n",
    "    try:\n",
    "        eta[key] = eta[key] + grad_estimate**2\n",
    "    except ValueError:\n",
    "        eta[key][sub_sample_image_index,:] = eta[key][sub_sample_image_index,:] + grad_estimate**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(Y,location_arr, K, eps):\n",
    "    '''\n",
    "    main SGD implementation, following steps in table 5\n",
    "    this doesn't implement hotspot initlization\n",
    "    Y: obsereved data\n",
    "    location_err: observed voxel location\n",
    "    K: number of sources\n",
    "    eps: tolerance parm\n",
    "    ---------------------------\n",
    "    Return: \n",
    "    alpha dict\n",
    "    '''\n",
    "    t = 0\n",
    "    maxStepSize = 1.0\n",
    "    N_sub = 10\n",
    "    V_sub = 5000\n",
    "    M = 50\n",
    "    max_delta = 1e10\n",
    "    gamma = 0.1 \n",
    "    N = Y.shape[0]\n",
    "    V = Y.shape[1]\n",
    "    pi = hyperparameter(location_arr)\n",
    "    alpha = initial_alpha(pi, N, K)\n",
    "    eta = initial_eta(alpha)\n",
    "    elbo = []\n",
    "    #while max_delta > eps:\n",
    "    while t<30: \n",
    "    #while t<5:\n",
    "        t += 1\n",
    "        print 'Iteration {}...'.format(t)\n",
    "        max_delta = 0.0\n",
    "        alpha_0 = deepcopy(alpha)\n",
    "        _, sub_sample_image_index, start_point = sub_sampling_mask(N, V, N_sub, V_sub)\n",
    "        M_samples = [sample_from_Q(alpha_0, sub_sample_image_index) for _ in np.arange(M)]\n",
    "        ##calculate ELBO\n",
    "        logPQ_dic = logPQ(M_samples,Y,location_arr,sub_sample_image_index,start_point,V_sub, pi,alpha_0)\n",
    "        elbo.append(ELBO(logPQ_dic))\n",
    "        print 'current elbo is {}'.format(elbo[(t-1)])\n",
    "        for key in alpha_0.keys():\n",
    "            print 'at alpha {}'.format(key)\n",
    "            rho = gamma/np.sqrt(eta[key])\n",
    "            grad_estimate = gradient_estimate(key,M_samples,logPQ_dic,alpha_0,Y,location_arr,sub_sample_image_index,start_point,V_sub, pi)\n",
    "            delta = delta_on_grad_estimate(grad_estimate, rho, maxStepSize, sub_sample_image_index)\n",
    "            update_alpha(key, alpha, alpha_0, delta, sub_sample_image_index)\n",
    "            update_eta(key, eta, grad_estimate, sub_sample_image_index)\n",
    "            #if key in ['mu_w_n_k', 'k_w_n_k']:\n",
    "            #    print np.max(rho[sub_sample_image_index,:])\n",
    "            #    print np.max(grad_estimate)\n",
    "            if np.max(np.abs(delta)) >= max_delta:\n",
    "                max_delta = np.max(np.abs(delta))\n",
    "            #    print max_delta\n",
    "        #max_delta = 0.0\n",
    "        if (not t%10):\n",
    "            save_cPickle('./data/alpha_{}.save'.format(t), alpha)        \n",
    "    return (alpha, eta,elbo)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def final_SGD(Y,location_arr, K, eps, alpha, eta):\n",
    "    '''\n",
    "    To ensure that all of the per-image source weights converge to local optima, \n",
    "    we set Nsub~N and maxStepSize \\infty, \n",
    "    fix all of the global parameters, and re-run the inference procedure until all of the local parameters converge.\n",
    "    '''\n",
    "    t = 0\n",
    "    maxStepSize = np.infty\n",
    "    N_sub = Y.shape[0]\n",
    "    V_sub = 5000\n",
    "    M = 50\n",
    "    max_delta = 1e10\n",
    "    gamma = 0.1 \n",
    "    N = Y.shape[0]\n",
    "    V = Y.shape[1]\n",
    "    pi = hyperparameter(location_arr)\n",
    "    while (max_delta > eps) & (t<5):\n",
    "        t += 1\n",
    "        print 'Final update iteration {}...'.format(t)\n",
    "        max_delta = 0.0\n",
    "        alpha_0 = deepcopy(alpha)\n",
    "        _, sub_sample_image_index, start_point = sub_sampling_mask(N, V, N_sub, V_sub)\n",
    "        M_samples = [sample_from_Q(alpha_0, sub_sample_image_index) for _ in np.arange(M)]\n",
    "        for key in ['mu_w_n_k', 'k_w_n_k']:\n",
    "            print 'Final update at alpha {}'.format(key)\n",
    "            rho = gamma/eta[key]\n",
    "            grad_estimate = gradient_estimate(key,M_samples,alpha_0,Y,location_arr,sub_sample_image_index,start_point,V_sub, pi)\n",
    "            delta = delta_on_grad_estimate(grad_estimate, rho, maxStepSize, sub_sample_image_index)\n",
    "            update_alpha(key, alpha, alpha_0, delta, sub_sample_image_index)\n",
    "            update_eta(key, eta, grad_estimate, sub_sample_image_index)\n",
    "            if np.max(np.abs(delta))> max_delta:\n",
    "                max_delta = np.max(np.abs(delta))\n",
    "        #max_delta = 0.0\n",
    "    return alpha\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_cPickle(save_path, object_dict):\n",
    "    '''\n",
    "    save a alpha dict into a pickel object in a give save_path\n",
    "    '''\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        cPickle.dump(object_dict, f, protocol=cPickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 2...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 3...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 4...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 5...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 6...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 7...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 8...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 9...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 10...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 11...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 12...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 13...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 14...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 15...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 16...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 17...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 18...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 19...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 20...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 21...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 22...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 23...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 24...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 25...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 26...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 27...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 28...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 29...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 30...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 31...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 32...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 33...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 34...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 35...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 36...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 37...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 38...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 39...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 40...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 41...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 42...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 43...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 44...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 45...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 46...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 47...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 48...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 49...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 50...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 51...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 52...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 53...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 54...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 55...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 56...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 57...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 58...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 59...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 60...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 61...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 62...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 63...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 64...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 65...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 66...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 67...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 68...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 69...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 70...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 71...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 72...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 73...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 74...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 75...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 76...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 77...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 78...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 79...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 80...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 81...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 82...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 83...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 84...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 85...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 86...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 87...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 88...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 89...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 90...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 91...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 92...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 93...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 94...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 95...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 96...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 97...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 98...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 99...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n",
      "Iteration 100...\n",
      "at alpha k_mu_k\n",
      "at alpha mu_mu_k\n",
      "at alpha mu_w_n_k\n",
      "at alpha mu_lambda_k\n",
      "at alpha k_w_n_k\n",
      "at alpha k_lambda_k\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    Y = np.load('data/Y.npy')\n",
    "    location_arr = np.load('data/location_arr.npy')\n",
    "    alpha, eta,elbo = SGD(Y,location_arr, 10, 0.01)\n",
    "    #alpha_final = final_SGD(Y,location_arr, 10, 0.01, alpha, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_cPickle('../../data/alpha_100_bad.save', alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k_lambda_k': array([ 1.68592408,  2.10500643, -0.27198432,  1.74348024,  1.82963878,\n",
       "         0.66182427,  1.86414624,  0.21980981,  2.4681977 ,  1.57592704]),\n",
       " 'k_mu_k': array([[-2.23040779, -2.59532382, -0.7452928 ],\n",
       "        [-2.26946407, -2.61900768, -0.61302435],\n",
       "        [-1.0849702 , -2.61227779, -0.62148988],\n",
       "        [-2.1948565 , -2.5888774 , -0.61391197],\n",
       "        [-2.2778247 , -2.61913195, -0.65960157],\n",
       "        [-2.19974658, -2.59429421, -0.71184   ],\n",
       "        [-2.31361282, -2.59579635, -0.77449361],\n",
       "        [-2.1920898 , -2.61516887, -0.66558463],\n",
       "        [-2.17736367, -2.35826727, -0.61260779],\n",
       "        [-2.26455732, -2.59652644, -0.5728484 ]]),\n",
       " 'k_w_n_k': array([[ 2.87515852,  1.26378502,  1.78216093, ...,  3.00580814,\n",
       "          3.25869708,  0.36657348],\n",
       "        [ 1.1526669 ,  3.32848148,  1.3655506 , ...,  3.42494664,\n",
       "          0.85526326,  1.50685977],\n",
       "        [ 2.30258509,  3.26428491,  3.25222668, ...,  1.26390649,\n",
       "          3.43032459,  1.33535167],\n",
       "        ..., \n",
       "        [ 0.49800107,  1.07615266,  1.07165948, ...,  3.39239913,\n",
       "          1.3855235 ,  2.05902693],\n",
       "        [ 2.22432966,  3.58817523,  3.20148795, ...,  1.3161248 ,\n",
       "          3.84604861,  1.4925052 ],\n",
       "        [ 3.26388146,  3.14146962,  3.32517729, ...,  1.04719017,\n",
       "          3.33436995,  3.20065672]]),\n",
       " 'mu_lambda_k': array([ 2.0460894 , -1.99281803,  2.04277892, -0.02313869,  0.68159894,\n",
       "         1.58848104,  3.80297307, -1.79118457,  3.45726526,  0.316888  ]),\n",
       " 'mu_mu_k': array([[  32.38253885,   45.19875903,   17.25996043],\n",
       "        [  17.30036849,   59.9776092 ,   -8.89278395],\n",
       "        [  -3.64439706,   39.33169751,   -7.86826345],\n",
       "        [  83.84057194,  105.01066064,  -16.90658713],\n",
       "        [  22.2728803 ,   67.77231334,   44.71636323],\n",
       "        [  59.20746643,  121.5871812 ,   19.57206101],\n",
       "        [  17.53348452,   79.32795206,   14.15967937],\n",
       "        [  77.95194975,   21.0302804 ,   32.82581722],\n",
       "        [  -7.34467108,   -4.42056702,  -16.88748543],\n",
       "        [  16.46109664,   81.93393608,   -5.25318493]]),\n",
       " 'mu_w_n_k': array([[-3.386961  , -1.61112992,  3.10391574, ...,  1.3926867 ,\n",
       "          0.75824534, -0.31335776],\n",
       "        [ 1.66494279,  4.29282644, -0.74560629, ...,  1.50843392,\n",
       "         -1.2274135 ,  0.39954523],\n",
       "        [-1.12355553, -4.89484606, -0.76945737, ..., -1.79856161,\n",
       "          0.62946004,  3.35431572],\n",
       "        ..., \n",
       "        [ 0.5753389 , -1.4622857 , -1.08006487, ...,  0.59450575,\n",
       "          1.23688614,  0.43403748],\n",
       "        [ 0.38000313,  4.18579864, -3.74118867, ...,  0.54018567,\n",
       "          1.44509855, -1.72658368],\n",
       "        [ 0.7522138 , -1.14972826,  2.7201683 , ..., -1.5930809 ,\n",
       "          2.50162373,  1.00809378]])}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
